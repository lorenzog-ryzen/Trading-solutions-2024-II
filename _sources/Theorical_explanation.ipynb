{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theorical Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Aspects of the Project\n",
    "\n",
    "### DAG (Directed Acyclic Graph):\n",
    "- Khans Topological order\n",
    "\n",
    "### Neural Networks\n",
    "- Backward propagation\n",
    "- Forward propagation\n",
    "\n",
    "### Genetic Algorithm\n",
    "- Genotype\n",
    "- Fenotype \n",
    "- Mutation\n",
    "- Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG\n",
    "Un DAG (Directed Acyclic Graph, o Grafo Dirigido Acíclico) es una estructura matemática que consta de nodos conectados por aristas (o enlaces), donde cada arista tiene una dirección específica y no existen ciclos. Es decir, si sigues las aristas en la dirección indicada, nunca podrás regresar al mismo nodo de partida, lo que garantiza que el grafo sea acíclico.\n",
    "![](images_lorenzo//net_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kahn's Topological Order\n",
    "Kahn's topological order is a method used to determine the sequence in which operations should be performed in a directed acyclic graph (DAG), which is particularly useful in neural networks and computational graphs. This algorithm finds the correct order to compute the forward propagation in a neural network by identifying dependencies between the nodes. Each node represents a mathematical operation or a neuron, and edges between nodes represent the flow of data.\n",
    "\\\n",
    "![](images_lorenzo//Topological-order.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Let $ G $ be a Directed Acyclic Graph (DAG), where $ G_A $ represents the input layer nodes, $ G_B $ the hidden layer nodes, and $ G_C $ the output layer nodes. From the perspective of the DAG, the network consists of only three layers. Let $ T $ represent the weights associated with the edges between nodes.\n",
    "\n",
    "The forward propagation for a neuron in the hidden layer $ G_B $ can be calculated as follows:\n",
    "\n",
    "Let $ i \\in (1, n) $, where $ n $ is the number of neurons in the hidden layer, and let $ p \\in (1, k) $, where $ k $ is the number of incoming edges (connections to the neuron).\n",
    "\n",
    "The activation of neuron $ GB_i $ in the hidden layer is given by:\n",
    "\n",
    "$$\n",
    "GB_i = \\tanh\\left( \\sum_{j=1}^{p} (T_{B_j} \\cdot \\text{normalized incoming information}) - \\text{bias per neuron} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation\n",
    "\n",
    "Backward propagation, often referred to as backpropagation, is a key algorithm used in training neural networks. It is responsible for updating the weights of the network based on the error between the predicted output and the actual target values. In the context of a Directed Acyclic Graph (DAG), backpropagation is executed in the reverse order of forward propagation.\n",
    "\n",
    "Let $ G $ be a Directed Acyclic Graph (DAG), where $ G_A $ represents the input layer nodes, $ G_B $ the hidden layer nodes, and $ G_C $ the output layer nodes. The weights associated with the edges are represented by $ T $.\n",
    "\n",
    "During backpropagation, the error is propagated backwards through the network, starting from the output layer $ G_C $ and moving towards the input layer $ G_A $. This process can be summarized as follows:\n",
    "\n",
    "1. **Calculate the Error at the Output Layer**: For each neuron $ GC_i $ in the output layer, compute the error $ E_i $ as the difference between the predicted output and the actual target value. This is often defined as:\n",
    "$$ E_i = \\text{target}_i - \\text{output}_i $$\n",
    "\n",
    "\n",
    "2. **Compute the Gradient for the Output Layer**: The gradient of the loss with respect to the output of the neuron is calculated using the derivative of the activation function. For example, using the tangent hyperbolic activation function:\n",
    "$$ \\delta_{C_i} = E_i \\cdot \\text{activation\\_derivative}(\\text{output}_i) $$\n",
    "where $ \\text{activation\\_derivative} $ is the derivative of the activation function.\n",
    "\n",
    "3. **Propagate the Error Backwards**: For each neuron $ GB_j $ in the hidden layer, compute the error signal $ \\delta_{B_j} $ as the weighted sum of the errors from the output layer:\n",
    "$$ \\delta_{B_j} = \\sum_{i=1}^{m} \\delta_{C_i} \\cdot T_{C_i \\to B_j} $$\n",
    "where $ m $ is the number of output neurons.\n",
    "\n",
    "4. **Update Weights**: After calculating the error signals for the hidden layer, the weights can be updated using a learning rate $ \\eta $:\n",
    "$$\n",
    "T_{B_j} = T_{B_j} + \\eta \\cdot \\delta_{B_j} \\cdot input \\to {G_{B_j}}\n",
    "   $$\n",
    "   The same process applies to update the weights connecting the input layer to the hidden layer.\n",
    "\n",
    "By repeating this process iteratively over multiple epochs, the neural network adjusts its weights to minimize the overall error, improving its ability to predict outcomes accurately.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
